{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOl/tgtHidLEkkl9FdXIovU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stanleykywu/ml-intro/blob/main/Primitive_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating your own Chat-GPT(ish)"
      ],
      "metadata": {
        "id": "ABstr2YpvaLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "Run the installation code block ONLY if you are running this in a Google Colab. Nothing will if you run it locally but hopefully you wouldn't need to since all packages will already have been installed"
      ],
      "metadata": {
        "id": "QtxCBTc5q8PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch\n",
        "%pip install transformers"
      ],
      "metadata": {
        "id": "laro468p_OvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary functions"
      ],
      "metadata": {
        "id": "kVB184n4u4-e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ukx4h5c8-Ukv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Dataset/Textual Corpus\n",
        "\n",
        "Define a textual corpus for your model help answer a specific question. In general, the more text, the better your model will perform. In reality, something like ChatGPT would be trained on upwards of 45TB of textual data."
      ],
      "metadata": {
        "id": "GxMWQkcQv7cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = r\"\"\"\n",
        "A transformer is a deep learning model that adopts the\n",
        "mechanism of attention, differentially weighing the\n",
        "significance of each part of the input data. It is used\n",
        "primarily in the field of natural language processing\n",
        "(NLP) and in computer vision (CV).\n",
        "\n",
        "Like recurrent neural networks (RNNs), transformers are \n",
        "designed to handle sequential input data, such as natural \n",
        "language, for tasks such as translation and text \n",
        "summarization. However, unlike RNNs, transformers do not\n",
        "necessarily process the data in order. Rather, the \n",
        "attention mechanism provides context for any position in \n",
        "the input sequence. \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "olx8EcDn-bk6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing a Tokenizer and Model for Fine-Tuning\n",
        "\n",
        "Rather than training a model from scratch, we fine-tune an existing language model, like BERT from Google. In particular, this one we use is good at answering question prompts."
      ],
      "metadata": {
        "id": "18-NsNbFwcGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "metadata": {
        "id": "GYtMm-f6-dJ-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asking our own ChatGPT a question"
      ],
      "metadata": {
        "id": "VrnP2FCxywxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How do transformers work?\"\n",
        "print(f\"The question:\\n{question}\")"
      ],
      "metadata": {
        "id": "ws40J_zk-f9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we package our textual corpus and question and feed it to our model. We then decode what the model outputs and returns it as text."
      ],
      "metadata": {
        "id": "ltXdQdlGytbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(question, corpus, add_special_tokens=True, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "outputs = model(**inputs)\n",
        "\n",
        "ans_start_scores = outputs.start_logits\n",
        "ans_end_scores = outputs.end_logits\n",
        "\n",
        "ans_start = torch.argmax(ans_start_scores)\n",
        "ans_end = torch.argmax(ans_end_scores) + 1\n",
        "\n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[ans_start:ans_end]))"
      ],
      "metadata": {
        "id": "3E27hdWi-ly3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The question:\\n-------------------------\\n{question}\\n\")\n",
        "print(f\"The answer:\\n-------------------------\\n{answer}\")"
      ],
      "metadata": {
        "id": "g-p2eb7p-mP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}